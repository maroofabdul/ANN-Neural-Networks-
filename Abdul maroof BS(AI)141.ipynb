{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING DATASET\n",
    "\n",
    "The Iris dataset is loaded from a CSV file using pandas. The first few rows are displayed to check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris_data = pd.read_csv( r'/home/shahzaib/Documents//iris.csv')\n",
    "print(iris_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Removing Dublicates\n",
    "\n",
    "Any duplicate rows in the dataset are removed to ensure the data is clean and free from redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed. New shape: (147, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iris_data = iris_data.drop_duplicates()\n",
    "print(\"Duplicates removed. New shape:\", iris_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Feature scaling\n",
    "\n",
    "The numeric features (sepal and petal dimensions) are scaled using StandardScaler to ensure they have a mean of 0 and a standard deviation of 1. This helps in improving the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature Scaled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "features = iris_data.iloc[:, :-1]  \n",
    "scaled_features = scaler.fit_transform(features)\n",
    "print(\"feature Scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Data Splitting\n",
    "\n",
    "The dataset is split into training and test sets (80% for training, 20% for testing). This prepares the data for training machine learning models and evaluating their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Splitited\n",
      "Training set shape: (117, 4) Test set shape: (30, 4)\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# 3. Data Splitting\n",
    "X = scaled_features  # Features\n",
    "y = iris_data['species']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Data Splitited\")\n",
    "print(\"Training set shape:\", X_train.shape, \"Test set shape:\", X_test.shape)\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The above codes  demonstrates basic data preprocessing steps using the Iris dataset. It covers the following key steps:\n",
    "\n",
    "    Loading the Dataset: The Iris dataset is loaded using pandas, and the initial few rows are displayed to verify the data.\n",
    "\n",
    "    Removing Duplicates: Duplicate rows are removed to ensure the dataset is clean, reducing redundancy.\n",
    "\n",
    "    Feature Scaling: The feature columns (sepal and petal measurements) are scaled using StandardScaler to standardize the data, improving the performance of machine learning models.\n",
    "\n",
    "    Data Splitting: The dataset is split into training (80%) and testing (20%) sets using train_test_split. This split allows for model training and evaluation.\n",
    "\n",
    "The preprocessing steps ensure the data is clean, normalized, and ready for use in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
